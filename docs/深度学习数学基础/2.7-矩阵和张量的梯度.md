

## 2.7-矩阵和张量的梯度

标量组成向量，向量进一步可以组成矩阵，矩阵再进一步组成了张量。所以现在我们进一步学习矩阵、张量的梯度（每一个位置求偏导）。

由于深度学习中的大部分运算的输入输出都是张量（tensor），比如输入的一张图像通常是$3 \times height \times width$，所以我们需要能够明白张量和张量之间的梯度是如何计算的。

类比输入输出都是向量的情况，假设函数$f: \mathbb R^{N_1 \times N_2 ... \times N_{D_x}} \to \mathbb R^{M_1 \times M_2 ... \times M_{D_y}}$的输入是一个$D_x$维的向量，形状是$N_1 \times N_2 ... \times N_{D_x}$，输出是$D_y$维度的向量$M_1 \times M_2 ... \times M_{D_y}$，对于这样的$y=f(x)$，**generalized Jacobian** $\frac{\partial y}{\partial x}$的形状是：
$$(M_1 \times M_2 ... \times M_{D_y}) \times (N_1 \times N_2 ... \times N_{D_x})$$

注意：我们将Jacobian矩阵的维度分成了两组：第一组是输出$y$的维度，第二组是输入$x$的形状。这样分组之后，我们可以将**generalized Jacobian**矩阵看成一个“假二维”的矩阵便于理解。这个“假二维”矩阵的每一行的形状与$x$相同，每一列的形状与$y$相同。

假设$i \in \mathbb Z ^{D_y}, j \in \mathbb Z ^{D_x}$ 是“假矩阵”的下标（下标不再是标量，而是一个向量来指明位置），我们有：

$$(\frac{\partial y}{\partial x})_{i,j} = \frac{\partial y_i}{\partial x_j} \tag{2.8.1}$$

式子(2.8.1)中的$y_i, x_j$都是标量，但$i,j$是向量，表示具体的$y$和$x$在张量中的位置， 因此$\frac{\partial y_i}{\partial x_j}$依旧是一个标量。有了上面的式子，我们进一步可以得到输入和输出的关系为：

$$x \to x + \Delta x \Rightarrow y \to \approx y + \frac{\partial y}{\partial x}  \Delta x \tag{2.8.2}$$

只不过此时$\Delta x$的维度是$N_1 \times N_2 ... \times N_{D_x}$, $\frac{\partial y}{\partial x}$的维度是: $(M_1 \times M_2 ... \times M_{D_y}) \times (N_1 \times N_2 ... \times N_{D_x})$, $\frac{\partial y}{\partial x}  \Delta x$的维度是：$M_1 \times M_2 ... \times M_{D_y}$。

式子(2.8.2)中的矩阵乘法和1.2章节中的矩阵乘法法则一样，暂且称之为**generalized matrix multiply**：

$$(\frac{\partial y}{\partial x}  \Delta x)_j = \sum_i (\frac{\partial y}{\partial x} )_{i,j} (\Delta x)_i = (\frac{\partial y}{\partial x} )_{j, :} \cdot \Delta x$$

和普通矩阵乘法唯一的不同就是下标$i,j$不再是标量，而是下标向量。$(\frac{\partial y}{\partial x} )_{j, :}$可以看作矩阵$\frac{\partial y}{\partial x}$的第$j$行，该行形状与$x$相同，因此直接和$\Delta x$进行elementwise乘法。

于是对于张量，我们同样有**链式法则**：假设$y=f(x), z=g(y)$, $x,y$形状如上，$z$的形状是$K_1,...K_{D_z}$,我们可以将张量链式法则写成：

$$\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y} \frac{\partial y}{\partial x}$$

区别在于$\frac{\partial z}{\partial x}$的维度是$(K_1 \times K_2 ... \times K_{D_z}) \times (N_1 \times N_2 ... \times N_{D_x})$,$\frac{\partial z}{\partial y}$的维度是$(K_1 \times K_2 ... \times K_{D_z}) \times (M_1 \times M_2 ... \times M_{D_y})$,$\frac{\partial y}{\partial x}$的维度是$(M_1 \times M_2 ... \times M_{D_y}) \times (N_1 \times N_2 ... \times N_{D_x})$。


从**generalized matrix multiply**：角度，我们可以得到：
$$(\frac{\partial z}{\partial x})_{i,j} = \sum_k (\frac{\partial z}{\partial x} )_{i,k} (\frac{\partial y}{\partial x})_{k,j} = (\frac{\partial z}{\partial y} )_{i, :} \cdot (\frac{\partial y}{\partial x} )_{:, j}$$