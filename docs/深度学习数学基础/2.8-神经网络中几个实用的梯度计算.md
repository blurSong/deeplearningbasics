

## 2.8-神经网络中几个实用的梯度计算

2.8章节我们知道了张量求梯度的方法以及张量中的链式法则，这一章节我们结合Jacobian矩阵继续学习神经网络中几个非常实用的梯度计算。

### 2.8.1 $z = Wx$矩阵$W \in \mathbb R^{n \times m}$乘以列向量$x \in \mathbb R^m$求$\frac{\partial z}{\partial x}$

可以看作函数将输入$x \in \mathbb R^m$ 经过$W \in \mathbb R^{n \times m}$变换得到输出$z \in \mathbb R^n$,那么Jacobian矩阵$\frac{\partial z}{\partial x} \in \mathbb R^{n \times m}$

$$z_i = \sum^{m}_k W_{ik}x_k$$

那么
$$(\frac{\partial z}{\partial x})_{i,j} = \frac{\partial z_i}{\partial x_j} = \frac{\partial {\sum^{m}_k W_{ik}x_k}}{\partial x} = \sum^{m}_k W_{ik} \frac{\partial x_k}{\partial x_j} = W_{ij}$$

由于$\frac{\partial x_k}{\partial x_j} = 1$ if $k = j$ else 0， 所以有$\frac{\partial z}{\partial x} = W$

### 2.8.2 $z=xW, \frac{\partial z}{\partial x} = W^T$

### 2.8.3 $z=x$向量等于自身,求$\frac{\partial z}{\partial x}$

因为$z_i = x_i$ 所以
$$(\frac{\partial z}{\partial x})_{i,j} = \frac{\partial z_i}{\partial x_j} = \frac{\partial x_i}{\partial x_j} = \{^{1, i=j}_{0, otherwise}$$

所以$\frac{\partial z}{\partial x} = I$,将其放在链式法则中进行矩阵乘法时候不会改变其他矩阵。

### 2.8.4 $z = f(x)$ 对向量$x$中每个元素进行变换, 求$\frac{\partial z}{\partial x}$
由于$z_i = f(x_i)$所以
$$(\frac{\partial z}{\partial x})_{i,j} = \frac{\partial z_i}{\partial x_j} = \frac{\partial f(x_i)}{\partial x_j} = \{^{f'(x_i), i=j}_{0, otherwise}$$

所以$\frac{\partial z}{\partial x}$是一个diagonal matrix 且$\frac{\partial z}{\partial x} = diag(f'(x))$

矩阵乘以一个diagonal矩阵也就是每个元素进行幅度变换，因此链式法则中的矩阵乘以$diag(f'(x))$相当于和$f'(x)$做elementwise 乘法。

### 2.8.5 $z = Wx, \delta = \frac{\partial J}{\partial z}$，求$\frac{\partial J}{\partial W}=\frac{\partial J}{\partial z} \frac{\partial z}{\partial W}=\delta \frac{\partial z}{\partial W}$

我们开始引入更复杂的情况，因为神经网络中往往包含多次链式法则的引用，这里我们假设已经知道$\delta = \frac{\partial J}{\partial z}$，直接求$\frac{\partial J}{\partial W}$。

假设神经网络的损失函数$J$是标量，我们想计算的是损失函数对参数$W \in \mathbb R^{n \times m}$的梯度。我们可以想象神经网络这个函数输入是一个$n \times m$形状的参数，输出是一个标量，结合上一章节Jacobian知识我们可以知道$\frac{\partial J}{\partial W} \in \mathbb R^{(1) \times (n \times m)}$形状和$W$一样，所以在神经网络训练的时候可以将参数减去参数的梯度乘以学习率。

根据链式法则，我们需要求出$\frac{\partial z}{\partial W} \in \mathbb R^{(n) \times (n \times m)}$。这个三维的张量不方便表示且十分复杂，因此我们先只看对$W_{i,j}$求导$\frac{\partial z}{\partial W_{i,j}} \in \mathbb R^{(n) \times (1)}$。

$$z_k = \sum^m_{l=1}  W_{kl}x_l$$
$$\frac{\partial z}{\partial W_{ij}} = \sum^m_{l=1}x_l \frac{\partial W_{kl}}{\partial W_{ij}}$$

$$\frac{\partial W_{kl}}{\partial W_{ij}} = \{^{1, i=k, j=l}_0$$

所以只有$i=k, j=l$时候非零
$$\frac{\partial z}{\partial W_{ij}} = \begin{bmatrix}
0\\ 
.\\ 
x_j\\ 
.\\ 
0
\end{bmatrix} \leftarrow ith element, j=l$$

所以
$$\frac{\partial J}{\partial W_{ij}} = \frac{\partial J}{\partial z}\frac{\partial z}{\partial W_{ij}} = \delta \frac{\partial z}{\partial W_{ij}} = \delta_i x_j$$

所以得到
$$\frac{\partial J}{\partial W} = \delta^T x^T $$

### 2.8.6 $z=xW, \delta = \frac{\partial J}{\partial z}, \frac{\partial J}{\partial W} =x^T\delta$

### 2.8.7 7. $\hat{y} =  softmax(\Theta), J=CrossEntropy(y,\hat{y}), \frac{\partial J}{\partial \Theta } = \hat{y} - y$

假设神经网络到达softmax之前的输出为$\Theta = [\theta_1, \theta_2, ..., \theta_n]$,$n$为分类数量，那么
$$\hat{y} = softmax(\Theta), \theta_i = \frac{e^{\theta_i}}{\sum^n_k e^{\theta_k}}$$
$$J = \sum^n_{i=1}y_ilog(\hat{y_i})$$
$$\frac{\partial J}{\partial{\hat{y_i}}} = \frac{y_i}{\hat{y_i}}$$
$$\frac{\partial \hat{y_i}}{\partial \theta_i} = \{^{\hat{y_i}(1-\hat{y_k}), i=k}_{-\hat{y_i}\hat{y_k}, i \neq k}$$

$$\frac{\partial J}{\partial{\theta_i}} = \frac{\partial J}{\hat{y}} \frac{\partial \hat{y}}{\partial{\theta_i}}, 形状为1 \times n, n \times 1$$

$$\frac{\partial J}{\partial{\theta_i}} = \sum^n_{k=1}{\frac{\partial J}{\hat{y_k}} \frac{\partial \hat{y_k}}{\partial{\theta_i}}} (k=i, k\neq i)$$

$$\frac{\partial J}{\partial{\partial{\theta_i}}} = \frac{\partial J}{\hat{y_i}} \frac{\partial \hat{y_i}}{\partial \theta_i} + \sum^n_{k \neq  i}{\frac{\partial J}{\hat{y_k}} \frac{\partial \hat{y_k}}{\theta_i}} (k=i, k\neq i)$$

$$\frac{\partial J}{\partial{\theta_i}} = y_i(1- \hat{y_i}) + \sum_{k neq i} {y_k \hat{y_i}}$$

$$\frac{\partial J}{\partial{\theta_i}} = -y_i(1- \hat{y_i}) + \sum_{k neq i} {y_k \hat{y_i}}$$

$$\frac{\partial J}{\partial{\theta_i}} = -y_i + \hat{y_i} \sum {y_k} $$

$$\frac{\partial J}{\partial{\theta_i}} = \hat{y_i} -y_i  $$

所以
$$\frac{\partial J}{\partial{\theta_i}} = \hat{y} -y \in \mathbb R^{n}  $$